DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'HOST': '',
        'PORT': '',
        'NAME': '{{ db_name }}',
        'USER': '{{ db_user }}',
        'PASSWORD': '{{ db_password }}',
    }
}

ALLOWED_HOSTS = ['pms', 'localhost']

BACKUP_LOCAL_DIRECTORY = '/path/to/backups' # Where to store local backups
BACKUP_DATABASE_COPIES = {
   'daily': 7,
   'weekly': 4,
   'monthly': 12,
}

SECRET_KEY = 'Enter something here...'

SYNCS = {
    # a command to pull the backups from server to localhost
    'pull_server_backups': 'rsync -rave ssh {server}:{backups_path} {local_backups_path}',

    # enable this locally and it will download read items from server
    'enable_pull_recent_read_info_items': True or False,

    # enable this locally and it will upload info items to server
    # enable for local setup where crawler runs, disable it on server and other setups.
    'enable_push_info_items': True or False,

    'sync_stocks_to_server': [
        'python manage.py dumpdata stocks --indent 4 > ~/stocks.json',
        'scp ~/stocks.json {server}:',
        'ssh {server} "cd {project_path}; python manage.py loaddata ~/stocks.json;"',
    ],
}

# Set this to True if this setup should run the info crawlers.
RUN_INFO_CRAWLERS = True or False

SERVER_EMAIL = 'admin@example.com'
EMAIL_HOST = 'smtp.example.com'
EMAIL_PORT = 25
EMAIL_HOST_USER = 'emailuser'
EMAIL_HOST_PASSWORD = 'password'
